{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4dlowv6dZWf",
        "outputId": "5ac84339-14a8-4c07-cc06-9142ead9f736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v3', 'LunarLanderContinuous-v3', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v3', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v1', 'CliffWalkingSlippery-v1', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Reacher-v5', 'Pusher-v2', 'Pusher-v4', 'Pusher-v5', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedPendulum-v5', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'InvertedDoublePendulum-v5', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'HalfCheetah-v5', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Hopper-v5', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Swimmer-v5', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Walker2d-v5', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Ant-v5', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'Humanoid-v5', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'HumanoidStandup-v5', 'GymV21Environment-v0', 'GymV26Environment-v0'])\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "from time import time\n",
        "from mlagents_envs.base_env import ActionTuple\n",
        "import gymnasium as gym\n",
        "from gymnasium import Wrapper\n",
        "from mlagents_envs.registry import default_registry\n",
        "from torchrl.data import ReplayBuffer, LazyTensorStorage\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import deque\n",
        "import torch.nn as nn\n",
        "from time import sleep, time\n",
        "import os\n",
        "import random\n",
        "import torchrl\n",
        "from gymnasium import envs\n",
        "#GPU support\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "print(envs.registry.keys())\n",
        "\n",
        "#constants + model_savefile\n",
        "LEARNING_RATE = 3e-4\n",
        "LEARNING_RATE_RESUME = 1.5e-4\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "STEPS_PER_EPOCH = 6000\n",
        "REPLAY_SIZE = 200000\n",
        "BATCH_SIZE = 256\n",
        "RESOLUTION = (84, 84)\n",
        "FRAME_REPEAT = 4\n",
        "TARGET_UPDATE_FREQUENCY = 250\n",
        "WARM_UP_STEPS = 5000\n",
        "SCALE = 100\n",
        "\n",
        "model_savefile = Path(\"./redq-checkpoints/redq-50\")\n",
        "model_savefile_latest = Path(\"./checkpoints/unity/latest\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for normalization of obs\n",
        "def compute_obs_stats(env, behavior_name, n_samples=5000):\n",
        "    obs_buffer = []\n",
        "    env.reset()\n",
        "    for _ in range(n_samples):\n",
        "        decision_steps, _ = env.get_steps(behavior_name)\n",
        "        for agent_id in decision_steps.agent_id:\n",
        "            obs = decision_steps[agent_id].obs\n",
        "            obs_flat = np.concatenate([o.flatten() for o in obs], axis=-1)\n",
        "            obs_buffer.append(obs_flat)\n",
        "        # Take random actions\n",
        "        for agent_id in decision_steps.agent_id:\n",
        "            action_dim = env.behavior_specs[behavior_name].action_spec.continuous_size\n",
        "            action = np.random.uniform(-1, 1, size=(action_dim,))\n",
        "            action_tuple = ActionTuple(continuous=np.expand_dims(action, axis=0))\n",
        "            env.set_action_for_agent(behavior_name, agent_id, action_tuple)\n",
        "        env.step()\n",
        "\n",
        "    obs_buffer = np.array(obs_buffer)\n",
        "    obs_mean = obs_buffer.mean(axis=0)\n",
        "    obs_std = obs_buffer.std(axis=0) + 1e-6\n",
        "    return obs_mean, obs_std\n",
        "#main run() function for running the env \n",
        "def run(game, agent, memory, num_epochs, steps_per_epoch=2000, behavior_name=None, load_model = False):\n",
        "    UTD_RATIO = 35\n",
        "    start_time = time()\n",
        "    \n",
        "    # If behavior_name is not provided, pick the first one(although we prob only use one behavior for this ex)\n",
        "    if behavior_name is None:\n",
        "        behavior_name = list(game.behavior_specs.keys())[0]\n",
        "    best_mean_score = -np.inf\n",
        "    if load_model:\n",
        "      obs_mean = np.load(model_savefile / \"obs_mean.npy\")\n",
        "      obs_std = np.load(model_savefile / \"obs_std.npy\")\n",
        "      print(\"obs_mean and obs_std loaded\")\n",
        "\n",
        "    else:\n",
        "      obs_mean, obs_std = compute_obs_stats(game, behavior_name)\n",
        "      np.save(model_savefile / \"obs_mean.npy\", obs_mean)\n",
        "      np.save(model_savefile / \"obs_std.npy\", obs_std)\n",
        "    print(f\"Obs mean and obs std computed as {obs_mean} and {obs_std}\")\n",
        "    print(f\"Training behavior: {behavior_name}\")\n",
        "    full_global_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        global_step = 0\n",
        "        episode_rewards = {}\n",
        "        game.reset()\n",
        "        train_scores = []\n",
        "\n",
        "        last_obs = {}\n",
        "        last_action = {}\n",
        "        while global_step < steps_per_epoch:\n",
        "            decision_steps, terminal_steps = game.get_steps(behavior_name)\n",
        "\n",
        "            for agent_id in decision_steps.agent_id:\n",
        "                obs = decision_steps[agent_id].obs\n",
        "                obs_flat = np.concatenate([o.flatten() for o in obs], axis=-1)\n",
        "                obs_norm = (obs_flat - obs_mean) / obs_std\n",
        "\n",
        "                action = agent.get_action(obs_norm)\n",
        "                action = np.expand_dims(action, axis=0)  # shape (1, act_dim)\n",
        "                action_tuple = ActionTuple(continuous=action)\n",
        "                game.set_action_for_agent(behavior_name, agent_id, action_tuple)\n",
        "\n",
        "                last_obs[agent_id] = obs_norm\n",
        "                last_action[agent_id] = action\n",
        "\n",
        "            game.step()\n",
        "            next_decisions, next_terminals = game.get_steps(behavior_name)\n",
        "\n",
        "            for agent_id in decision_steps.agent_id:\n",
        "                if agent_id in next_decisions.agent_id:\n",
        "                    n_obs = next_decisions[agent_id].obs\n",
        "                    done = False\n",
        "                    reward = next_decisions[agent_id].reward\n",
        "                elif agent_id in next_terminals.agent_id:\n",
        "                    n_obs = next_terminals[agent_id].obs\n",
        "                    done = True\n",
        "                    reward = next_terminals[agent_id].reward\n",
        "                else:\n",
        "                  continue\n",
        "\n",
        "                n_obs_flat = np.concatenate([o.flatten() for o in n_obs], axis=-1)\n",
        "                n_obs_norm = (n_obs_flat - obs_mean) / obs_std\n",
        "\n",
        "                memory.add({\n",
        "                    \"obs\": torch.tensor(last_obs[agent_id], dtype=torch.float32).unsqueeze(0),\n",
        "                    \"action\": torch.tensor(last_action[agent_id], dtype=torch.float32),  # no unsqueeze here\n",
        "                    \"reward\": torch.tensor(reward, dtype=torch.float32).unsqueeze(0),\n",
        "                    \"next_obs\": torch.tensor(n_obs_norm, dtype=torch.float32).unsqueeze(0),\n",
        "                    \"done\": torch.tensor(done, dtype=torch.bool).unsqueeze(0)\n",
        "                })\n",
        "\n",
        "        # Episode tracking\n",
        "                episode_rewards.setdefault(agent_id, 0)\n",
        "                episode_rewards[agent_id] += reward\n",
        "                if done:\n",
        "                    train_scores.append(episode_rewards[agent_id])\n",
        "                    episode_rewards[agent_id] = 0\n",
        "                    last_obs.pop(agent_id, None)\n",
        "                    last_action.pop(agent_id, None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # SAC updates\n",
        "            if len(memory) > BATCH_SIZE and full_global_step > WARM_UP_STEPS:\n",
        "                for _ in range(UTD_RATIO):\n",
        "                    metrics = agent.update()\n",
        "\n",
        "                agent.update_target_networks()\n",
        "                if full_global_step % 500 == 0:\n",
        "                    print(f\"Step {full_global_step} | Buffer: {len(memory)}/{REPLAY_SIZE} | \"\n",
        "                          f\"Alpha: {metrics['alpha']:.3f} | \"\n",
        "                          f\"Actor Loss: {metrics['actor_loss']:.3f} | \"\n",
        "                          f\"Critic Loss: {metrics['critic_loss']:.3f}\")\n",
        "            global_step += 1\n",
        "            full_global_step += 1\n",
        "\n",
        "        # Save model periodically\n",
        "        # Save best separately\n",
        "\n",
        "        if len(train_scores) > 0 and np.mean(train_scores) > best_mean_score:\n",
        "            agent.save_model(model_savefile / \"best\")\n",
        "            best_mean_score = np.mean(train_scores)\n",
        "            print(f\"New best model saved at epoch {epoch} (mean={best_mean_score:.2f})\")\n",
        "\n",
        "# always save latest(uneeded if running without time limit but helpful to have a fallback)\n",
        "        if epoch % 2 == 0:\n",
        "            agent.save_model(model_savefile / \"latest\")\n",
        "        else:\n",
        "            agent.save_model(model_savefile / \"latest2\")\n",
        "        \n",
        "        # Logging\n",
        "        if len(train_scores) > 0:\n",
        "            print(\n",
        "                f\"[{behavior_name}] Epoch {epoch} | \"\n",
        "                f\"Mean: {np.mean(train_scores):.1f} +/- {np.std(train_scores):.1f} | \"\n",
        "                f\"Min: {np.min(train_scores):.1f} | Max: {np.max(train_scores):.1f}\"\n",
        "            )\n",
        "\n",
        "        elapsed = (time() - start_time) / 60.0\n",
        "        print(f\"Total elapsed time for epoch {epoch}: {elapsed:.2f} minutes\\n\")\n",
        "\n",
        "    game.close()\n",
        "    return agent, game\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#storage for memoryBuffer(we don't use prio replay since REDQ performs \n",
        "# lots of updates anyways and using prio replay could destablize by training over and over again on bad runs)\n",
        "storage = LazyTensorStorage(REPLAY_SIZE)\n",
        "memoryBuffer = ReplayBuffer(storage=storage)\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, in_channels, action_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(in_channels, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256)\n",
        "        )\n",
        "        self.mean_layer = nn.Linear(256, action_dim)\n",
        "        self.log_std_layer = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        mean = self.mean_layer(x)\n",
        "        log_std = self.log_std_layer(x)\n",
        "        log_std = torch.clamp(log_std, min=-20, max=2) #clamp it to ensure stability\n",
        "        #return mean log_std to define prob distribution which we sample from in sample()\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state):\n",
        "        mean, log_std = self.forward(state)\n",
        "        std = torch.exp(log_std)\n",
        "        normal = torch.distributions.Normal(mean, std)\n",
        "        z = normal.rsample()   # use rsample for reparameterization, this way grad. can be computed, without this grad can't pass through\n",
        "        action = torch.tanh(z)\n",
        "        # log prob with tanh correction\n",
        "        log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
        "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
        "        return action, log_prob, z\n",
        "\n",
        "\n",
        "class REDQCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        input_dim = state_dim + action_dim\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 512),  nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state = state.view(state.size(0), -1)\n",
        "        action = action.view(action.size(0), -1)\n",
        "        #concat state and action for getting how good action + state is\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        q = self.linear(x)\n",
        "        return q\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class REDQ(nn.Module): #REDQ\n",
        "    def __init__(self, action_dim, in_channels, buffer, batch_size = BATCH_SIZE, discount_factor = DISCOUNT_FACTOR):\n",
        "        super().__init__()\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.003\n",
        "        self.lr = 3e-4\n",
        "        self.buffer = buffer\n",
        "        self.Actor = Actor(in_channels, action_dim)\n",
        "        self.batch_size = batch_size\n",
        "        self.ActorOptim = torch.optim.Adam(self.Actor.parameters(), lr = self.lr)\n",
        "        self.critics = [REDQCritic(in_channels, action_dim) for _ in range(10)]\n",
        "        self.criticTargets = [REDQCritic(in_channels, action_dim) for _ in range(10)]\n",
        "        self.Optims = [torch.optim.Adam(critic.parameters(), lr = LEARNING_RATE) for critic in self.critics]\n",
        "        self.Actor.to(DEVICE)\n",
        "        #high exploration, normal REDQ uses -action_dim \n",
        "        self.target_entropy = -action_dim * 0.3\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=DEVICE)\n",
        "        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=3e-4)\n",
        "        for critic in self.critics:\n",
        "          critic.to(DEVICE)\n",
        "        for target in self.criticTargets:\n",
        "          target.to(DEVICE)\n",
        "\n",
        "        for critic, target in zip(self.critics, self.criticTargets):\n",
        "          target.load_state_dict(critic.state_dict())\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
        "        action, _, _ = self.Actor.sample(state)\n",
        "        return action.detach().cpu().numpy()[0]\n",
        "    @property\n",
        "    def alpha(self):\n",
        "        return self.log_alpha.exp()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        sampled_data = self.buffer.sample(self.batch_size)\n",
        "        observations = sampled_data[\"obs\"]  # Current states\n",
        "        actions = sampled_data[\"action\"]  # Actions taken\n",
        "        rewards = sampled_data[\"reward\"]  # Rewards received\n",
        "        next_observations = sampled_data[\"next_obs\"]  # Next states\n",
        "        dones = sampled_data[\"done\"]\n",
        "\n",
        "        observations = observations.to(DEVICE)\n",
        "        actions = actions.to(DEVICE)\n",
        "        rewards = rewards.to(DEVICE)\n",
        "        next_observations = next_observations.to(DEVICE)\n",
        "        dones = dones.to(DEVICE)\n",
        "        observations = observations.view(observations.size(0), -1)\n",
        "        next_observations = next_observations.view(next_observations.size(0), -1)\n",
        "        actions = actions.view(actions.size(0), -1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_action, next_log_prob, _ = self.Actor.sample(next_observations)\n",
        "            lenTargets = len(self.criticTargets)\n",
        "            subsetSize = 2\n",
        "            subset = random.sample(range(lenTargets), subsetSize)\n",
        "            q_values = []\n",
        "            for index in subset:\n",
        "                q = self.criticTargets[index](next_observations, next_action)\n",
        "                q_values.append(q)\n",
        "            q_values = torch.stack(q_values, dim=0)\n",
        "            q_value = q_values.min(dim = 0)[0]\n",
        "            next_v = q_value - self.alpha * next_log_prob\n",
        "            target_q = rewards + (1 - dones.float()) * self.gamma * next_v\n",
        "\n",
        "            target_q = target_q.detach()\n",
        "\n",
        "        new_action, log_prob, _ = self.Actor.sample(observations)\n",
        "        #alpha loss\n",
        "        alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n",
        "        self.alpha_optimizer.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.alpha_optimizer.step()\n",
        "        #critic loss\n",
        "        qs = [critic(observations, actions) for critic in self.critics]\n",
        "        losses = [F.mse_loss(q, target_q) for q in qs]\n",
        "        for opt, loss in zip(self.Optims, losses):\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(opt.param_groups[0]['params'], max_norm=1.0)\n",
        "            opt.step()\n",
        "\n",
        "\n",
        "        q_new_actions = [critic(observations, new_action) for critic in self.critics]\n",
        "        q_new_actions = torch.stack(q_new_actions, dim = 0).mean(dim=0)\n",
        "        #actor loss\n",
        "\n",
        "        actor_loss = (self.alpha * log_prob - q_new_actions).mean()\n",
        "        self.ActorOptim.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.Actor.parameters(), max_norm=1.0)\n",
        "        self.ActorOptim.step()\n",
        "        return {\n",
        "          'alpha': self.alpha.item(),\n",
        "          'actor_loss': actor_loss.item(),\n",
        "          'critic_loss': torch.stack(losses).mean().item()\n",
        "        }\n",
        "        # Update target networks\n",
        "\n",
        "    def update_target_networks(self):\n",
        "        with torch.no_grad():\n",
        "            for critic, targets in zip(self.critics, self.criticTargets):\n",
        "                for target_param, param in zip(targets.parameters(), critic.parameters()):\n",
        "                    target_param.mul_(1 - self.tau)\n",
        "                    target_param.add_(self.tau * param)\n",
        "\n",
        "    def save_model(self, model_savefile):\n",
        "        for i in range (len(self.critics)):\n",
        "            torch.save(self.critics[i].state_dict(), model_savefile / f\"Critic{i}.pth\")\n",
        "            torch.save(self.criticTargets[i].state_dict(), model_savefile / f\"CriticTargets{i}.pth\")\n",
        "            torch.save(self.Optims[i].state_dict(), model_savefile / f\"OptimCritic{i}.pth\")\n",
        "        torch.save(self.Actor.state_dict(), model_savefile / \"Actor.pth\")\n",
        "        torch.save(self.log_alpha, model_savefile / \"log_alpha.pth\")\n",
        "        torch.save(self.ActorOptim.state_dict(), model_savefile / \"ActorOptim.pth\")\n",
        "        torch.save(self.alpha_optimizer.state_dict(), model_savefile / \"OptimAlpha.pth\")\n",
        "        \n",
        "    \n",
        "        \n",
        "\n",
        "    def load_model(self, model_input):\n",
        "        #can replace with /latest if needed\n",
        "\n",
        "        model_input = Path(model_input)\n",
        "        print(f\"Loading model from: {model_input}\")\n",
        "\n",
        "        self.Actor.load_state_dict(torch.load(model_input / \"Actor.pth\", map_location=DEVICE))\n",
        "        self.ActorOptim.load_state_dict(torch.load(model_input / \"OptimActor.pth\", map_location=DEVICE))\n",
        "        log_alpha_path = model_input / \"log_alpha.pth\"\n",
        "        loaded = torch.load(log_alpha_path, map_location=DEVICE)\n",
        "        self.log_alpha = loaded.to(DEVICE).detach()\n",
        "        self.log_alpha.requires_grad = True\n",
        "        optim_alpha_path = model_input / \"OptimAlpha.pth\"\n",
        "        self.alpha_optimizer.load_state_dict(torch.load(optim_alpha_path, map_location=DEVICE))\n",
        "                \n",
        "        for i in range(len(self.critics)):\n",
        "            critic_path = model_input / f\"Critic{i}.pth\"\n",
        "            target_path = model_input / f\"CriticTargets{i}.pth\"\n",
        "            self.critics[i].load_state_dict(torch.load(critic_path, map_location=DEVICE))\n",
        "            self.criticTargets[i].load_state_dict(torch.load(target_path, map_location=DEVICE))\n",
        "            optim_path = model_input / f\"OptimCritic{i}.pth\"\n",
        "            self.Optims[i].load_state_dict(torch.load(optim_path, map_location=DEVICE))\n",
        "\n",
        "        print(f\"Model load complete from {model_input}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for training\n",
        "load_model = False\n",
        "skip_learning = False\n",
        "env_id = \"Crawler\"\n",
        "env = default_registry[env_id].make(worker_id=3, no_graphics = True)  # Specify a unique worker_id cuz i have no clue how to shut down the env will look into it\n",
        "env.reset()\n",
        "behavior_name = list(env.behavior_specs)[0]\n",
        "spec = env.behavior_specs[behavior_name]\n",
        "action_dim = spec.action_spec.continuous_size\n",
        "in_channels = sum(np.prod(o.shape) for o in spec.observation_specs)\n",
        "agent = REDQ(action_dim, in_channels, memoryBuffer)\n",
        "agent.to(DEVICE)\n",
        "if load_model:\n",
        "    agent.load_model()\n",
        "if not skip_learning:\n",
        "    agent, game = run(\n",
        "        env,\n",
        "        agent,\n",
        "        memoryBuffer,\n",
        "        num_epochs= 14,\n",
        "        steps_per_epoch=STEPS_PER_EPOCH,\n",
        "        behavior_name = behavior_name,\n",
        "        load_model = load_model\n",
        "    )\n",
        "\n",
        "    print(\"======================================\")\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
            "    \"memorysetup-bucket-allocator-granularity=16\"\n",
            "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
            "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
            "    \"memorysetup-bucket-allocator-block-count=1\"\n",
            "    \"memorysetup-main-allocator-block-size=16777216\"\n",
            "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
            "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
            "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
            "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
            "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
            "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
            "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
            "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
            "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
            "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
            "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
            "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
            "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
            "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
            "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
            "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
            "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
            "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
            "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
            "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
            "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
            "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
            "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
            "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
            "Loading saved model weights...\n",
            "Loading model from: redq-checkpoints/redq-50\n",
            "Model load complete from redq-checkpoints/redq-50\n",
            "Model loaded successfully.\n",
            "Number of observation specs: 2\n",
            "  Obs 0: shape=(126,), type=ObservationType.DEFAULT\n",
            "  Obs 1: shape=(32,), type=ObservationType.DEFAULT\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     agent_rewards\u001b[38;5;241m.\u001b[39msetdefault(agent_id, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     49\u001b[0m     agent_rewards[agent_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m terminal_steps[agent_id]\u001b[38;5;241m.\u001b[39mreward\n\u001b[0;32m---> 51\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(decision_steps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(terminal_steps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/unity/lib/python3.10/site-packages/mlagents_envs/timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/unity/lib/python3.10/site-packages/mlagents_envs/environment.py:348\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m step_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_step_input(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_actions)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicator.exchange\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexchange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityCommunicatorStoppedException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunicator has exited.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/unity/lib/python3.10/site-packages/mlagents_envs/rpc_communicator.py:142\u001b[0m, in \u001b[0;36mRpcCommunicator.exchange\u001b[0;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[1;32m    140\u001b[0m message\u001b[38;5;241m.\u001b[39munity_input\u001b[38;5;241m.\u001b[39mCopyFrom(inputs)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39msend(message)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll_for_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39mrecv()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/unity/lib/python3.10/site-packages/mlagents_envs/rpc_communicator.py:106\u001b[0m, in \u001b[0;36mRpcCommunicator.poll_for_timeout\u001b[0;34m(self, poll_callback)\u001b[0m\n\u001b[1;32m    104\u001b[0m callback_timeout_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout_wait \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m<\u001b[39m deadline:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munity_to_external\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_timeout_wait\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;66;03m# Got an acknowledgment from the connection\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m poll_callback:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;66;03m# Fire the callback - if it detects something wrong, it should raise an exception.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/unity/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/unity/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
            "File \u001b[0;32m~/miniconda3/envs/unity/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
            "File \u001b[0;32m~/miniconda3/envs/unity/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "episodes_to_watch = 10\n",
        "env_id = \"Crawler\"\n",
        "env = default_registry[env_id].make(worker_id=12)\n",
        "env.reset()\n",
        "behavior_name = list(env.behavior_specs)[0]\n",
        "spec = env.behavior_specs[behavior_name]\n",
        "action_dim = spec.action_spec.continuous_size\n",
        "in_channels = sum(np.prod(o.shape) for o in spec.observation_specs)\n",
        "agent = REDQ(action_dim, in_channels, memoryBuffer)\n",
        "agent.to(DEVICE)\n",
        "print(\"Loading saved model weights...\")\n",
        "agent.load_model(model_savefile) \n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "agent.Actor.eval()\n",
        "for critic in agent.critics:\n",
        "    critic.eval()\n",
        "    \n",
        "obs_mean = np.load(model_savefile / \"obs_mean.npy\")\n",
        "obs_std = np.load(model_savefile / \"obs_std.npy\")\n",
        "\n",
        "#for watching\n",
        "spec = env.behavior_specs[behavior_name]\n",
        "print(f\"Number of observation specs: {len(spec.observation_specs)}\")\n",
        "for i, obs_spec in enumerate(spec.observation_specs):\n",
        "    print(f\"  Obs {i}: shape={obs_spec.shape}, type={obs_spec.observation_type}\")\n",
        "\n",
        "max_steps = 3000\n",
        "for i in range(episodes_to_watch):\n",
        "    env.reset()\n",
        "    agent_rewards = {}\n",
        "    for step in range(max_steps):\n",
        "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
        "        for agent_id in decision_steps.agent_id:\n",
        "            obs = decision_steps[agent_id].obs\n",
        "            obs_flat = np.concatenate([o.flatten() for o in obs])\n",
        "            obs_norm = (obs_flat - obs_mean) / obs_std\n",
        "\n",
        "            action = agent.get_action(obs_norm)\n",
        "            action = np.expand_dims(action, axis=0)\n",
        "            action = ActionTuple(continuous=action)\n",
        "            env.set_action_for_agent(behavior_name, agent_id, action)\n",
        "            agent_rewards.setdefault(agent_id, 0)\n",
        "            reward = decision_steps[agent_id].reward\n",
        "            agent_rewards[agent_id] += reward\n",
        "\n",
        "        for agent_id in terminal_steps:\n",
        "            agent_rewards.setdefault(agent_id, 0)\n",
        "            agent_rewards[agent_id] += terminal_steps[agent_id].reward\n",
        "        \n",
        "        env.step()\n",
        "\n",
        "        if len(decision_steps) == 0 and len(terminal_steps) == 0:\n",
        "            break\n",
        "    \n",
        "    rewards = list(agent_rewards.values())\n",
        "    print(f\"Episode {i} | Mean: {np.mean(rewards):.1f} | \"\n",
        "          f\"Max: {np.max(rewards):.1f} | Min: {np.min(rewards):.1f}\")\n",
        "\n",
        "print(\"Watching ended\")\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Parameters: 308,993\n",
            "Trainable Parameters: 308,993\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of SACCritic\n",
        "critic_instance = SACCritic(state_dim=in_channels, action_dim=action_dim)\n",
        "actor_instance = Actor(state_dim = in_channels, action_dim = action_dim)\n",
        "# Calculate total and trainable parameters\n",
        "total_params = sum(p.numel() for p in critic_instance.parameters())\n",
        "trainable_params = sum(p.numel() for p in critic_instance.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready to create new environment\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "# Force garbage collection to clean up any lingering environment references\n",
        "gc.collect()\n",
        "time.sleep(2)  # Give it a moment to clean up\n",
        "\n",
        "print(\"Ready to create new environment\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "unity",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
